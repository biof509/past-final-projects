{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arman Yashar Khojandi\n",
    "#FAES BIOF509: Applied Machine Learning\n",
    "#Profs Alexander Goncearenco & Ayal Gussow\n",
    "#Due: 23 December 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "For this project, I analyzed a subset of 82 subjects (aged 22-35) from the WU-Minn Human Connectome Project 1200 Subjects Release 7T fMRI movie-watching protocol. \n",
    "-  Each subject had 4 total \"sessions\" where they watched video clips (no real-life footage, 2-4 minutes each, separated by 20 seconds of no stimulus). \n",
    "-  Each session had four such clips. \n",
    "-  All subjects completed self-report and standard-measurement psychological/emotional/cognitive tests, and behavioral measures were recorded and provided by HCP.\n",
    "-  I preprocessed the data and compared the performance of sk-learn's PLS and Ridge regression procedures in predicting subjects' behavioral measures based on their neural responses to the stimuli--broken into 1.7 million cubic measures of brain space, hereafter termed \"voxels.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some context\n",
    "-  For this project I only used 82 subjects (total 184), 2 video clips, and 3 behaviors in analysis\n",
    "-  The video clips differed greatly in content\n",
    "> -  \"Dreary\" consisted of rainy, desolate scenes without dialogue, humans, or a narrative, and containing eerie music\n",
    "> -  \"Bridgeville\" was a clip about oldtown USA wherein many people were interviewed and described their lifestyles and traditions, somewhat nostalgic and happy in tone, sentimental music in the background\n",
    "-  The three behaviors I used were scores of \n",
    "> -  \"Sadness_Unadj\" (self-reported), a measure of how sad subjects were\n",
    "> -  \"EmotSupp_Unadj\" (self-reported, a measure of how confident subjects were that people in their lives would support them emotionally/cared about them)\n",
    "> -  \"PMAT24_A_CR\" (standardized) a measure of fluid intelligence; how many correct responses they had on the PMAT24\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing & ML stuff I did here\n",
    "-  HCP provided the data with ICA-FIX denoising and preprocessing already performed \n",
    "-  I performed spatial blurring of the data (1.6mm^3 voxels --> 4.0mm^3 voxels)\n",
    "-  Initial subject timeseries were read in as (113, 136, 113, [time]) matrices--once steps were performed time axis collapsed\n",
    "-  For each clip, I created a total sum of all subject timseries\n",
    "> -  I then excluded each subject from the total and calculated the correlation between that subject and the mean of the subject-excluded total sum\n",
    "- I concatenated the resulting data for all subjects (per clip)\n",
    "- These data were read in below under variable name \"rnd1_fname\" and input to the ML models as \"X\" after the important data were extracted\n",
    "- The vector of behavioral measures for all subjects was read in as \"y\"\n",
    "> -  1 subject did not have data for \"Dreary\" clip, so 81 subjects were used for that one\n",
    "- I performed a standard scalar and transform procedure on data beforehand for better visualization/interpretation of results\n",
    "- Transformed X was then fit to transformed y using cross-validation (cross_val_score and cross_val_predict used to determine relationship and predict y from X.\n",
    "- Finally, I plotted y_observed vs y_predicted\n",
    "- CV R-squared values from each fold were saved to output files, as were the output vectors of y_obs, y_preds\n",
    "- I duplicated the for loop for Ridge regression and PLS because at the time I was experiencing performance issues and wanted to parallelize so to speak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  NOTE: In the second pass after initial results, following z-scoring, I weighted each subject's data by his/her behavioral measure of interest to see the models were better able to fit features to subjs--that is why there are two versions for each plot of clipxbehavior in my powerpoint\n",
    "- the ML steps were the same as below, however\n",
    "- I also copy pasted this from a couple of very ugly, cluttered notebooks, so I apologize if some small snippet of code is missing--you can rest assured I did it since I got results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "from scipy.stats import linregress, zscore\n",
    "from matplotlib.axes import Axes\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in behavioral measures to be used as y\n",
    "behav_data = pd.read_csv('/data/HCP_preproc/just_numeric_measures.csv', index_col=['Subject'])\n",
    "behav_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing small subset of behaviors and clips that I will analyze\n",
    "behaviors = [ \"Sadness_Unadj\", \"EmotSupp_Unadj\", \"PMAT24_A_CR\" ]\n",
    "listclips = [ \"bridgeville\", \"dreary\"]\n",
    "\n",
    "print(listclips)\n",
    "print(behaviors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine learnin' time! ...Will the cognition engine boot up?\n",
    "\n",
    "g = open(clipdir + \"PLS_outputs.txt\", 'a')\n",
    "for clip in listclips:\n",
    "    \n",
    "    #Grabbing file of interest\n",
    "    rnd1_fname = \"{}{}.nii\".format(clipdir, clip) # read in nifti file--in the second pass this was something like \"\"{}{}_{}_corrcoef_wmean_stacked.npy\" --a 2D array of brain data for all subjects\n",
    "    rnd1_nb = nib.load(rnd1_fname) #This was the step that caused performance issues--nibabel and nilearn will memory map for larger files unless told mmap=False, causing read/write issues known to exist on the latest versions of helix/felix\n",
    "    rnd1_img_data = nib.load(rnd1_fname).get_fdata() #Extract data\n",
    "    x, y, z, subjlen = rnd1_img_data.shape #get dimensions for later\n",
    "    #Redone for sanity check later\n",
    "    img_data = rnd1_nb.get_fdata()\n",
    "    i_size, j_size, k_size, t = img_data.shape\n",
    "    \n",
    "    X = img_data.reshape(t,-1) # this gets the feature matrix into the correct shape (subjects x features)\n",
    "    \n",
    "    #Loop for each behavioral measure--Austin Powers reference!\n",
    "    for oh_behave in behaviors:\n",
    "        y = behav_data[oh_behave]\n",
    "        y = y[0:82]\n",
    "        \n",
    "        #In the case of \"dreary\" which only has data for 81 subjects\n",
    "        if subjlen == 81:\n",
    "            y = y.drop(index=65) #not super elegant/flexible--I changed this later to drop data for any subject who didn't have the neuroimaging data\n",
    "        else:\n",
    "            y = y[0:82]\n",
    "        y = np.reshape(np.array(y),(-1,1)) # this gets the target matrix into the correct shape (subjects x target(s))\n",
    "        print(\"The shape of X is \" + str(X.shape))\n",
    "        print(\"The shape of y is \" + str(y.shape))\n",
    "        \n",
    "        #Define scaler/normalized data\n",
    "        scalar = preprocessing.StandardScaler()\n",
    "\n",
    "        X_scaler = preprocessing.StandardScaler().fit(X)\n",
    "        y_scaler = preprocessing.StandardScaler().fit(y)\n",
    "\n",
    "        X_scaled = X_scaler.transform(X) \n",
    "        y_scaled = y_scaler.transform(y)\n",
    "        \n",
    "        #Output to file\n",
    "        g.write(\"Clip is {} \\n\".format(clip))\n",
    "        g.write(\"Behavior is {} \\n\".format(oh_behave))\n",
    "        g.flush()\n",
    "        \n",
    "        #Partial least squares regression\n",
    "        pls2 = PLSRegression(n_components=1)\n",
    "        pls2.fit(X_scaled,y_scaled)\n",
    "        pls2_R2 = pls2.score(X_scaled,y_scaled)\n",
    "        print(\"pls2 R^2 = {}\".format(pls2_R2))\n",
    "        g.write(\"PLS R^2 = {} \\n\".format(pls2_R2))\n",
    "        g.flush()\n",
    "        \n",
    "        pls2_pipeline = Pipeline([('transformer', scalar), ('estimator', pls2)]) #set up the pipeline for less clutter\n",
    "        \n",
    "        #Run the model with tenfold cross validation\n",
    "        pls_cross_scores = cross_val_score(pls2_pipeline, X, y, cv=10, scoring=\"r2\")\n",
    "        g.write(\"PLS cross val scores are {} \\n\".format(pls_cross_scores))\n",
    "        g.flush()\n",
    "        \n",
    "        #Predict y\n",
    "        pls_pred = cross_val_predict(pls2_pipeline, X, y, cv=10)\n",
    "        g.write(\"PLS predicted values are {} \\n\".format(pls_pred))\n",
    "        pls_pred_z = zscore(pls_pred)\n",
    "        g.write(\"Normalized PLS predicted values are: {} \\n\".format(pls_pred_z))\n",
    "        g.flush()\n",
    "        \n",
    "        #Plot the results\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(y_scaled, pls_pred_z, edgecolors=(0, 0, 0))        \n",
    "        plt.axis('scaled')\n",
    "        axes = plt.gca()\n",
    "        \n",
    "        #Try to doctor the axis scales a bit to look nicer\n",
    "        if (y_scaled.max() - y_scaled.min()) > (pls_pred_z.max() - pls_pred_z.min()):\n",
    "            ax.set_ylim([y_scaled.min() - 1, y_scaled.max() + 1])\n",
    "            ax.set_xlim([y_scaled.min() - 1, y_scaled.max() + 1])\n",
    "        else:\n",
    "            ax.set_ylim([pls_pred_z.min() - 1, pls_pred_z.max() + 1])\n",
    "            ax.set_xlim([pls_pred_z.min() - 1, pls_pred_z.max() + 1])\n",
    "        #axes.set_aspect('equal', adjustable='box', anchor='C')\n",
    "        #axes.set_anchor('C')\n",
    "        \n",
    "        #Get stats to add R^2 value in legend--this wasn't done in the first pass but I added it here since I'm not marking down the later versions\n",
    "        slope, intercept, rval, pval, stderr = linregress(y_scaled[:,0], pls_pred_z[:,0])\n",
    "        ax.plot(y_scaled, slope*y_scaled[:,0] + intercept, 'k--', label='R^2: {0:.2f}'.format(rval), lw=2)\n",
    "        plt.legend()\n",
    "        ax.set_title(\"PLS {}\".format(clip))\n",
    "        ax.set_xlabel(\"{} measured\".format(oh_behave))\n",
    "        ax.set_ylabel(\"{} predicted\".format(oh_behave))\n",
    "        plt.show()\n",
    "        fig.savefig(\"{}/PLS_{}_{}\".format(clipdir, clip, oh_behave))\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same method but with Ridge regression (assigned lambda as well as chosen via cross-val procedure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = open(\"{}/new_alphas_10_100_1000_clf_and_clfCV_results.txt\".format(clipdir), \"a\")\n",
    "for clip in listclips:\n",
    "    rnd1_fname = \"{}{}.nii\".format(clipdir, clip)\n",
    "    rnd1_nb = nib.load(rnd1_fname)\n",
    "    rnd1_img_data = nib.load(rnd1_fname).get_fdata()\n",
    "    x, y, z, subjlen = rnd1_img_data.shape\n",
    "    img_data = rnd1_nb.get_fdata()\n",
    "    i_size, j_size, k_size, t = img_data.shape\n",
    "    X = img_data.reshape(t,-1) # this gets the feature matrix into the correct shape (subjects x features)\n",
    "    for oh_behave in behaviors:\n",
    "        y = behav_data[oh_behave]\n",
    "        y = y[0:82]\n",
    "        if subjlen == 81:\n",
    "            y = y.drop(index=65)\n",
    "        else:\n",
    "            y = y[0:82]\n",
    "        y = np.reshape(np.array(y),(-1,1)) # this gets the target matrix into the correct shape (subjects x target(s))\n",
    "        print(\"The shape of X is \" + str(X.shape))\n",
    "        print(\"The shape of y is \" + str(y.shape))\n",
    "        \n",
    "        #Define scaler/normalized data\n",
    "        scalar = preprocessing.StandardScaler()\n",
    "\n",
    "        X_scaler = preprocessing.StandardScaler().fit(X)\n",
    "        y_scaler = preprocessing.StandardScaler().fit(y)\n",
    "\n",
    "        X_scaled = X_scaler.transform(X) \n",
    "        y_scaled = y_scaler.transform(y)\n",
    "        \n",
    "        #Output to file\n",
    "        h.write(\"Clip is {} \\n\".format(clip))\n",
    "        h.write(\"Behavior is {} \\n\".format(oh_behave))\n",
    "        h.flush()\n",
    "            \n",
    "        #CLF Ridge regression    \n",
    "        clf = Ridge(alpha=100)\n",
    "        clf.fit(X_scaled, y_scaled)\n",
    "        clf_R2 = clf.score(X_scaled,y_scaled)\n",
    "        print(\"clf R^2 = {}\".format(clf_R2))\n",
    "        h.write(\"CLF R^2 = {} \\n\".format(clf_R2))\n",
    "        h.flush()\n",
    "        \n",
    "        clf_pipeline = Pipeline([('transformer', scalar), ('estimator', clf)])\n",
    "        \n",
    "        clf_cross_scores = cross_val_score(clf_pipeline, X, y, cv=10, scoring=\"r2\")\n",
    "        h.write(\"CLF cross val scores are: {} \\n\".format(clf_cross_scores))\n",
    "        h.flush()\n",
    "        \n",
    "        clf_pred = cross_val_predict(clf_pipeline, X, y, cv=10)\n",
    "        h.write(\"CLF predicted values are: {} \\n\".format(clf_pred))\n",
    "        clf_pred_z = zscore(clf_pred)\n",
    "        h.write(\"Normalized CLF predicted values are: {} \\n\".format(clf_pred_z))\n",
    "        h.flush()\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(y_scaled, clf_pred_z, edgecolors=(0, 0, 0))\n",
    "        plt.axis('scaled')\n",
    "        axes = plt.gca()\n",
    "        if (y_scaled.max() - y_scaled.min()) > (clf_pred_z.max() - clf_pred_z.min()):\n",
    "            ax.set_ylim([y_scaled.min() - 1, y_scaled.max() + 1])\n",
    "            ax.set_xlim([y_scaled.min() - 1, y_scaled.max() + 1])\n",
    "        else:\n",
    "            ax.set_ylim([clf_pred_z.min() - 1, clf_pred_z.max() + 1])\n",
    "            ax.set_xlim([clf_pred_z.min() - 1, clf_pred_z.max() + 1])\n",
    "        #axes.set_aspect('equal', adjustable='box', anchor='C')\n",
    "        #axes.set_anchor('C')\n",
    "        slope, intercept, rval, pval, stderr = linregress(y_scaled[:,0], clf_pred_z[:,0])\n",
    "        ax.plot(y_scaled, slope*y_scaled[:,0] + intercept, 'k--', label='R^2: {0:.2f}'.format(rval), lw=2)\n",
    "        plt.legend()\n",
    "        ax.set_title(\"CLF_Ridge {}\".format(clip))\n",
    "        ax.set_xlabel(\"{} measured\".format(oh_behave))\n",
    "        ax.set_ylabel(\"{} predicted\".format(oh_behave))\n",
    "        plt.show()\n",
    "        fig.savefig(\"{}/clf_{}_{}\".format(clipdir, clip, oh_behave))\n",
    "        \n",
    "         #CLF Ridge regression cross-validated\n",
    "        clfCV = RidgeCV(alphas=np.array([ 10., 100., 1000. ]), cv=10)\n",
    "        clfCV.fit(X_scaled, y_scaled)\n",
    "        clfCV_R2 = clfCV.score(X_scaled,y_scaled)\n",
    "        print(\"clfCV R^2 = {}\".format(clfCV_R2))\n",
    "        h.write(\"CLFCV R^2 is {} \\n\".format(clfCV_R2))\n",
    "        h.flush()\n",
    "        \n",
    "        clfCV_pipeline = Pipeline([('transformer', scalar), ('estimator', clfCV)])\n",
    "        \n",
    "        cross_scores = cross_val_score(clfCV_pipeline, X, y, cv=10, scoring=\"r2\")\n",
    "        h.write(\"CLFCV cross val scores: {} \\n\".format(cross_scores))\n",
    "        h.flush()\n",
    "        \n",
    "        clfCV_pred = cross_val_predict(clfCV_pipeline, X, y, cv=10)\n",
    "        h.write(\"clfCV predicted values: {} \\n\".format(clfCV_pred))\n",
    "        clfCV_pred_z = zscore(clfCV_pred)\n",
    "        h.write(\"Normalized clfCV predicted values are: {} \\n\".format(clfCV_pred_z))\n",
    "        h.flush()\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(y_scaled, clfCV_pred_z, edgecolors=(0, 0, 0))\n",
    "        plt.axis('scaled')\n",
    "        axes = plt.gca()\n",
    "        if (y_scaled.max() - y_scaled.min()) > (clfCV_pred_z.max() - clfCV_pred_z.min()):\n",
    "            ax.set_ylim([y_scaled.min() - 1, y_scaled.max() + 1])\n",
    "            ax.set_xlim([y_scaled.min() - 1, y_scaled.max() + 1])\n",
    "        else:\n",
    "            ax.set_ylim([clfCV_pred_z.min() - 1, clfCV_pred_z.max() + 1])\n",
    "            ax.set_xlim([clfCV_pred_z.min() - 1, clfCV_pred_z.max() + 1])\n",
    "        #axes.set_aspect('equal', adjustable='box', anchor='C')\n",
    "        #axes.set_anchor('C')\n",
    "        slope, intercept, rval, pval, stderr = linregress(y_scaled[:,0], clfCV_pred_z[:,0])\n",
    "        ax.plot(y_scaled, slope*y_scaled[:,0] + intercept, 'k--', label='R^2: {0:.2f}'.format(rval), lw=2)\n",
    "        plt.legend()\n",
    "        ax.set_title(\"CLF_RidgeCV_{}\".format(clip))\n",
    "        ax.set_xlabel(\"{} measured\".format(oh_behave))\n",
    "        ax.set_ylabel(\"{} predicted\".format(oh_behave))\n",
    "        plt.show()\n",
    "        fig.savefig(\"{}/clfCV_{}_{}\".format(clipdir, clip, oh_behave))\n",
    "h.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second pass with behavior-weighted mean\n",
    "Because it's Christmas, I've included the second run through of the data that produced better results, with some apparent differences in the code, though the \"high-flow logic\" is mostly unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupdir = '/data/HCP_preproc/7T_movie/SubjectData/GroupResults/'\n",
    "behaviors = [ 'EmotSupp_Unadj', 'Sadness_Unadj', 'PMAT24_A_CR' ]\n",
    "listclips = [ 'dreary', 'bridgeville' ]\n",
    "\n",
    "h = open(\"{}/weighted_mean_alphas_10_100_1000_ridge_and_ridgeCV_results.txt\".format(groupdir), \"a\") #output file for values\n",
    "for clip in listclips:\n",
    "    #rnd1_fname = \"{}weighted_mean_{}_{}_stacked_corr.nii\".format(corrdir, clip, oh_behave) #stacked correlation matrix for all subjects\n",
    "    #rnd1_nb = nib.load(rnd1_fname)\n",
    "    #rnd1_img_data = nib.load(rnd1_fname).get_fdata()\n",
    "    #x, y, z, subjlen = rnd1_img_data.shape\n",
    "    #img_data = rnd1_nb.get_fdata()\n",
    "    #i_size, j_size, k_size, t = img_data.shape\n",
    "    #X = img_data.reshape(t,-1) # this gets the feature matrix into the correct shape (subjects x features)\n",
    "    for oh_behave in behaviors:\n",
    "        X = np.load(\"{}{}_{}_corrcoef_wmean_stacked.npy\".format(groupdir, clip, oh_behave))\n",
    "        brain, subjlen = X.shape\n",
    "        \n",
    "\n",
    "        print(\"Shape of corrcoef matrix is ({},{})\".format(brain, subjlen))\n",
    "        y = behav_data[oh_behave]\n",
    "        y = y[0:82]\n",
    "        if subjlen == 81:\n",
    "            y = y.drop(index=65)\n",
    "        else:\n",
    "            y = y[0:82]\n",
    "        y = np.reshape(np.array(y),(-1,1)) # this gets the target matrix into the correct shape (subjects x target(s))\n",
    "        print(\"The shape of X is \" + str(X.shape))\n",
    "        print(\"The shape of y is \" + str(y.shape))\n",
    "     \n",
    "        #X = X.astype(np.float)\n",
    "        #X = X[np.isfinite(X)]\n",
    "        #y = y.astype(np.float)\n",
    "\n",
    "        print(type(X))\n",
    "        print(type(y))\n",
    "        \n",
    "        #Define scaler/normalized data\n",
    "        scalar = preprocessing.StandardScaler()\n",
    "\n",
    "        X_scaler = preprocessing.StandardScaler().fit(X)\n",
    "        y_scaler = preprocessing.StandardScaler().fit(y)\n",
    "\n",
    "        X_scaled = X_scaler.transform(X) \n",
    "        y_scaled = y_scaler.transform(y)\n",
    "        \n",
    "        #Output to file\n",
    "        h.write(\"Clip is {} \\n\".format(clip))\n",
    "        h.write(\"Behavior is {} \\n\".format(oh_behave))\n",
    "        h.flush()\n",
    "            \n",
    "        #CLF Ridge regression    \n",
    "        clf = Ridge(alpha=100)\n",
    "        clf.fit(X_scaled, y_scaled)\n",
    "        clf_R2 = clf.score(X_scaled,y_scaled)\n",
    "        print(\"clf R^2 = {}\".format(clf_R2))\n",
    "        h.write(\"CLF R^2 = {} \\n\".format(clf_R2))\n",
    "        h.flush()\n",
    "        \n",
    "        #Run ridge pipeline and cross validate and predict\n",
    "        clf_pipeline = Pipeline([('transformer', scalar), ('estimator', clf)])\n",
    "        \n",
    "        clf_cross_scores = cross_val_score(clf_pipeline, X, y, cv=10, scoring=\"r2\")\n",
    "        h.write(\"CLF cross val scores are: {} \\n\".format(clf_cross_scores))\n",
    "        h.flush()\n",
    "        \n",
    "        clf_pred = cross_val_predict(clf_pipeline, X, y, cv=10)\n",
    "        h.write(\"CLF predicted values are: {} \\n\".format(clf_pred))\n",
    "        clf_pred_z = zscore(clf_pred)\n",
    "        h.write(\"Normalized CLF predicted values are: {} \\n\".format(clf_pred_z))\n",
    "        h.flush()\n",
    "        \n",
    "        #Plot observed vs predicted values\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(y_scaled, clf_pred_z, edgecolors=(0, 0, 0))\n",
    "        plt.axis('scaled')\n",
    "        axes = plt.gca()\n",
    "        if (y_scaled.max() - y_scaled.min()) > (clf_pred_z.max() - clf_pred_z.min()):\n",
    "            ax.set_ylim([y_scaled.min() - 1, y_scaled.max() + 1])\n",
    "            ax.set_xlim([y_scaled.min() - 1, y_scaled.max() + 1])\n",
    "        else:\n",
    "            ax.set_ylim([clf_pred_z.min() - 1, clf_pred_z.max() + 1])\n",
    "            ax.set_xlim([clf_pred_z.min() - 1, clf_pred_z.max() + 1])\n",
    "        #axes.set_aspect('equal', adjustable='box', anchor='C')\n",
    "        #axes.set_anchor('C')\n",
    "        slope, intercept, rval, pval, stderr = linregress(y_scaled[:,0], clf_pred_z[:,0])\n",
    "        ax.plot(y_scaled, slope*y_scaled[:,0] + intercept, 'k--', label='R^2: {0:.2f}'.format(rval), lw=2)\n",
    "        plt.legend()\n",
    "        ax.set_title(\"CLF_Ridge {}\".format(clip))\n",
    "        ax.set_xlabel(\"{} measured\".format(oh_behave))\n",
    "        ax.set_ylabel(\"{} predicted\".format(oh_behave))\n",
    "        plt.show()\n",
    "        fig.savefig(\"{}/weighted_mean_ridge_{}_{}.png\".format(groupdir, clip, oh_behave)) #save plot\n",
    "        \n",
    "        #CLF Ridge regression cross-validated\n",
    "        clfCV = RidgeCV(alphas=np.array([ 10., 100., 1000. ]), cv=10)\n",
    "        clfCV.fit(X_scaled, y_scaled)\n",
    "        clfCV_R2 = clfCV.score(X_scaled,y_scaled)\n",
    "        print(\"clfCV R^2 = {}\".format(clfCV_R2))\n",
    "        h.write(\"CLFCV R^2 is {} \\n\".format(clfCV_R2))\n",
    "        h.flush()\n",
    "        \n",
    "        clfCV_pipeline = Pipeline([('transformer', scalar), ('estimator', clfCV)])\n",
    "        \n",
    "        cross_scores = cross_val_score(clfCV_pipeline, X, y, cv=10, scoring=\"r2\")\n",
    "        h.write(\"CLFCV cross val scores: {} \\n\".format(cross_scores))\n",
    "        h.flush()\n",
    "        \n",
    "        clfCV_pred = cross_val_predict(clfCV_pipeline, X, y, cv=10)\n",
    "        h.write(\"clfCV predicted values: {} \\n\".format(clfCV_pred))\n",
    "        clfCV_pred_z = zscore(clfCV_pred)\n",
    "        h.write(\"Normalized clfCV predicted values are: {} \\n\".format(clfCV_pred_z))\n",
    "        h.flush()\n",
    "        \n",
    "        #Plot observed vs. predicted values\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(y_scaled, clfCV_pred_z, edgecolors=(0, 0, 0))\n",
    "        plt.axis('scaled')\n",
    "        axes = plt.gca()\n",
    "        if (y_scaled.max() - y_scaled.min()) > (clfCV_pred_z.max() - clfCV_pred_z.min()):\n",
    "            ax.set_ylim([y_scaled.min() - 1, y_scaled.max() + 1])\n",
    "            ax.set_xlim([y_scaled.min() - 1, y_scaled.max() + 1])\n",
    "        else:\n",
    "            ax.set_ylim([clfCV_pred_z.min() - 1, clfCV_pred_z.max() + 1])\n",
    "            ax.set_xlim([clfCV_pred_z.min() - 1, clfCV_pred_z.max() + 1])\n",
    "        #axes.set_aspect('equal', adjustable='box', anchor='C')\n",
    "        #axes.set_anchor('C')\n",
    "        slope, intercept, rval, pval, stderr = linregress(y_scaled[:,0], clfCV_pred_z[:,0])\n",
    "        ax.plot(y_scaled, slope*y_scaled[:,0] + intercept, 'k--', label='R^2: {0:.2f}'.format(rval), lw=2)\n",
    "        plt.legend()\n",
    "        ax.set_title(\"CLF_RidgeCV_{}\".format(clip))\n",
    "        ax.set_xlabel(\"{} measured\".format(oh_behave))\n",
    "        ax.set_ylabel(\"{} predicted\".format(oh_behave))\n",
    "        plt.show()\n",
    "        fig.savefig(\"{}/weighted_mean_clfCV_{}_{}.png\".format(groupdir, clip, oh_behave)) #save plot\n",
    "h.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
